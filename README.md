The amount of solid resources out there regarding Red Teaming AI in a practical way is a bit limited. There's a lot of vendor specific resources regarding AI, but most of the blogs and research out there only cover the basics and most of the academic research (some /w & some w/o PapersWithCode) are highly technical and are foriegn to most, unless you've been in the AI/ML community for some time and/or have an advanced degree in ML. 

What's listed below is a list of what I've identifed (in my opinion) as valuable in terms of resources regarding red teaming AI

Offsec ML Playbook
A database of offensive ML TTPâ€™s, broken down by supply chain attacks, offensive ML techniques and adversarial ML. The playbook aims to simplify the decision making process of targetting ML in an organization.

https://5stars217.github.io/2023-10-26-introducing-offsec-ml-framework/

LLM security is the investigation of the failure modes of LLMs in use, the conditions that lead to them, and their mitigations.

Here are links to large language model security content - research, papers, and news - posted by @llm_sec

https://llmsecurity.net/

Chat with LLMs - https://chat.lmsys.org/

AWS Sagemaker

https://docs.aws.amazon.com/pdfs/sagemaker/latest/dg/sagemaker-dg.pdf#page01      

LLM testing findings

https://github.com/BishopFox/llm-testing-findings/tree/main

A curation of awesome tools, documents and projects about LLM Security.

https://github.com/corca-ai/awesome-llm-security

Official Code for "Baseline Defenses for Adversarial Attacks Against Aligned Language Models

https://github.com/neelsjain/baseline-defenses

GitHub Copilot Chat: From Prompt Injection to Data Exfiltration

https://kai-greshake.de/

https://embracethered.com/blog/

Some notable 2024 blogs from ^ 

https://embracethered.com/blog/posts/2024/llm-context-pollution-and-delayed-automated-tool-invocation/
https://embracethered.com/blog/posts/2024/github-copilot-chat-prompt-injection-data-exfiltration/
https://embracethered.com/blog/posts/2024/whoami-conditional-prompt-injection-instructions/
https://embracethered.com/blog/posts/2024/machine-learning-attack-series-keras-backdoor-model/

Awesome-Backdoor-in-Deep-Learning: This GitHub repository provides a curated list of papers and resources on backdoor attacks and defenses in deep learning. It includes information on backdoor attacks in various contexts, such as image classification and language models, and links to relevant papers and code 

https://github.com/zihao-ai/Awesome-Backdoor-in-Deep-Learning

Offensive AI Compilation: This repository offers a collection of resources related to offensive AI, including tools for performing backdoor attacks. It includes frameworks like ART, Cleverhans, and TextAttack, which support various attack types and data formats

https://github.com/jiep/offensive-ai-compilation

Backdoor Learning Resources: This GitHub repository lists multiple resources related to backdoor learning, including papers, tools, and datasets. It covers various types of backdoor attacks and defenses, providing a comprehensive overview of the field 

https://github.com/THUYimingLi/backdoor-learning-resources

Chat-Models-Backdoor-Attacking: This repository contains code for implementing backdoor attacks on chat models. It includes methods for training and deploying chat models with distributed trigger-based backdoor attacks, which are designed to be triggered by specific user inputs across different conversation rounds 

https://github.com/hychaochao/Chat-Models-Backdoor-Attacking

Label Consistent Backdoor: This repository focuses on creating models with imperceptible triggers using adversarial perturbations. It includes a detailed methodology for implementing backdoor attacks using the CIFAR-10 dataset and the ResNet-18 model 

https://github.com/AhmadSavaiz03/Label_Consistent_Backdoor

These resources should provide you with a good starting point for understanding and implementing backdoor attacks in AI models.

https://owasp.org/www-project-top-10-for-large-language-model-applications/llm-top-10-governance-doc/LLM_AI_Security_and_Governance_Checklist-v1.1.pdf

https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf

https://www.microsoft.com/en-us/msrc/aibugbar

https://atlas.mitre.org/matrices/ATLAS/

For white paper summerizations and analysis 

https://raw.githubusercontent.com/pjcampbe11/Red-Teaming-AI/main/README.md 

The resources listed here and the efforts made to summerize the academic side of things is an-ongoing effort and any new content discovered or white papers analyzed, will be documented here.
